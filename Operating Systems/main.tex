\documentclass{report}
\usepackage[a4paper, portrait, top=20mm, left=20mm, right=20mm, bottom=20mm]{geometry}
\usepackage{graphicx,wrapfig,amsmath,amssymb,algorithm,algpseudocode,caption,subcaption} 

\title{Digital Image Processing}
\author{Abhijit Amrendra Kumar}
\date{August 2023}

\begin{document}

\maketitle

\chapter{Image Warping and Alignment}
\section{Basics}

\noindent A digital image, which is a version of the visual stimulus sampled at discrete locations with discretized values, can be regarded as a function $I = f(x,y)$, where $(x,y)$ are the spatial integer coordinates in typically a rectangular domain $\Omega = [0,W-1]\times[0,H-1]$. Each ordered pair $(x,y)$ denote a pixel, which is generally a square. Pixel dimensions relate to the spatial resolution of the light-collecting sensor of a camera. \\

\noindent The actual visual signal is analog, but digital cameras capture a discrete version of it, and also quantize the intensity values. In a typical grayscale image, the intensity values lie from $0\rightarrow 255$

\begin{center}
  \includegraphics[scale=0.5]{"images/dip-01.png"}
\end{center}

\section{Alignment}

\noindent Consider images $I_1,I_2$ of a scene acquired through different viewpoints. $I_1$ and $I_2$ are said to be aligned iff for every $(x,y)$ in the domain $\Omega$, the pixels at $(x,y)$ in $I_1$ and $I_2$ are in physical correspondence. \\

\noindent If not, the images are said to be misalgined with respect to each other, or we say there is relative motion between the images. Image alignment is the process of correcting for the relative motion between $I_1$ and $I_2$.

\section{Motion Models}

\noindent Let us denote the coordinates in $I_1$ as $(x_1,y_1)$, and those in $I_2$ as $(x_2,y_2)$. \\

\noindent Let's first consider \textbf{translation}.

\begin{align}
  \forall (x_1,y_1) \in \Omega, & \exists t_x,t_y, x_2 = x_1 + t_x, y_2 = y_1 + t_y \\
  \implies \begin{bmatrix}
             x_2 \\
             y_2 \\
             1
           \end{bmatrix}
                                & = \begin{bmatrix}
                                      1 & 0 & t_x \\
                                      0 & 1 & t_y \\
                                      0 & 0 & 1
                                    \end{bmatrix} \begin{bmatrix}
                                                    x_1 \\
                                                    y_1 \\
                                                    1
                                                  \end{bmatrix}
\end{align}

Next, let's consider rotation about a point $(x_c,y_c)$ anti-clockwise through angle $\theta$

\begin{align}
  x_2 & = (x_1-x_c)\cos\theta - (y_1-y_c)\sin\theta + x_c \\
  y_2 & = (x_1-x_c)\sin\theta + (y_1-y_c)\cos\theta + y_c \\
      & \implies \begin{bmatrix}
                   x_2 \\
                   y_2 \\
                   1
                 \end{bmatrix}
  = \begin{bmatrix}
      \cos\theta & -\sin\theta & x_c \\
      \sin\theta & \cos\theta  & y_c \\
      0          & 0           & 1
    \end{bmatrix} \begin{bmatrix}
                    x_1-x_c \\
                    y_1-y_c \\
                    1
                  \end{bmatrix}
\end{align}

\noindent Combining the two motions, we obtain
\begin{align}
  x_2 & = (x_1-x_c)\cos\theta - (y_1-y_c)\sin\theta + t_x \\
  y_2 & = (x_1-x_c)\sin\theta + (y_1-y_c)\cos\theta + t_y \\
      & \implies \begin{bmatrix}
                   x_2 \\
                   y_2 \\
                   1
                 \end{bmatrix}
  = \begin{bmatrix}
      \cos\theta & -\sin\theta & t_x \\
      \sin\theta & \cos\theta  & t_y \\
      0          & 0           & 1
    \end{bmatrix} \begin{bmatrix}
                    x_1-x_c \\
                    y_1-y_c \\
                    1
                  \end{bmatrix}
\end{align}

\noindent If the $2\times 2$ matrix $\begin{bmatrix}
    A_{11} & A_{12} \\
    A_{21} & A_{22} \\
  \end{bmatrix}$ is rank-deficient, we term the transformation as an \textbf{affine transformation}, since it will transform 2D figures into a line/point. \\

\noindent \textbf{Note}: Composition of multiple types of motion is given by the multiplication of their corresponding matrices. However, most motion compositions are not commutative.

\begin{center}
  \includegraphics[scale=0.8]{"images/dip-02.png"}
  \includegraphics[scale=0.9]{"images/dip-03.png"}
\end{center}

\section{Image Alignment: Control Points}

\noindent This method involves marking pairs of physically corresponding points on two images, either manually by an expert or automatically using geometric properties. Then using these points, the transformation matrix can be computed, and the whole image can be transformed to align with the original image.

\begin{center}
  \includegraphics[scale=1]{"images/dip-04.png"}
\end{center}

\noindent However, this method is always not feasible, because
\begin{itemize}
  \item it requires manual intervention
  \item it is error prone
  \item there are automatic methods like the SIFT technique to finding matching control points.
\end{itemize}

\section{Image Alignment: Mean Squared Error}

\noindent The mean squared error between 2 images is given by
$$
  \text{MSSD} = \frac{1}{N} \underset{x,y\in\Omega}{\sum} (I_1(x,y)-I_2(x,y))^2, \ N = \#\text{pixels in field of view}
$$

\noindent We can modify the equation in the following manner to find the transform matrix $T$
$$
  T = \text{argmin MSSD}_T (I_1(v), I_2(Tv)), \quad
  v = \begin{bmatrix}
    x \\
    y \\
    1
  \end{bmatrix}
$$

\end{document}

% --------------------------------

% \section{Introduction}
% \subsection{What is a Perceptron ?}
% \noindent Perceptron is a probabilistic model for information storage and organization in the brain.

% \begin{figure}[H]
% \centering
% \begin{subfigure}{.5\linewidth}
%   \centering
%   \includegraphics[width=0.8\linewidth]{"images/perc-03.jpg"}
%   \label{fig:sub1}
% \end{subfigure}%
% \begin{subfigure}{.5\linewidth}
%   \centering
%   \includegraphics[width=0.5\linewidth]{"images/perc-04.jpg"}
%   \label{fig:sub2}
% \end{subfigure}
% \end{figure}


% \noindent Mathematically, a perceptron is a function that takes input signals and produces an output signal based on the weighted sum of the inputs. These weights are adjusted during learning to enable the perceptron to make accurate classifications or decisions. \\

% \noindent The learning rule is inspired by Hebbian learning, a biological learning principle that suggests that synaptic connections between neurons strengthen when those neurons are activated together. In the context of the perceptron, the learning rule updates the weights to minimize the error between the perceptron's output and the desired target output. \\

% \noindent Frank Rosenblatt demonstrated that the perceptron can successfully learn linearly separable patterns and achieve convergence within a finite number of iterations. Therefore, the perceptron model can be used for binary classification tasks, where it learns to separate input data points into two classes by finding an optimal decision boundary. Although this model is not used in practice nowadays, it serves as a building block for the modern neural network architectures. \\

% \noindent However, Rosenblatt also highlighted the limitations of the perceptron model. He showed that the perceptron is limited to learning linearly separable patterns and cannot handle more complex patterns that require nonlinear decision boundaries. This limitation, known as the \textbf{perceptron convergence theorem}, sparked subsequent research into developing more advanced neural network architectures capable of handling nonlinear problems.

% \subsection{Key Assumptions}
% \begin{itemize}
%     \item Stimuli which are similar will tend to form pathways to same sets of response cells
%     \item Stimuli which are dissimilar will tend to form pathways to different sets of response cells
%     \item Application of positive and negative reinforcements may facilitate or hinder the formation of connections.
%     \item Similarity of stimuli is a dynamically evolving attribute.
% \end{itemize}

% \section{Perceptron Training}

% \subsection{Definitions}
% \begin{itemize}
%     \item \textbf{Margin of a hyperplane}: The distance between the hyperplane and the data point nearest to it.
%     \item \textbf{Margin of a dataset $\gamma$}: The maximum margin possible for that dataset using any weight vector $w$.
% \end{itemize}

% \subsection{Geometric Intuition}
% Given a set of points on a n-dimensional plane, each having one out of two labels $\{+1,-1\}$. 

% \noindent For visualization, we can use the following figure.
% \begin{center}
%     \includegraphics[scale=0.4]{"images/perc-01.png"}
% \end{center}

% \noindent We want to use the perceptron model to determine a hyperplane that can separate the points based on their labels, i.e. $\{+1,-1\}$.

% \subsection{Assumptions}
% Based on the above problem, for such a hyperplane to exist, the dataset must satisfy some assumptions.
% \begin{itemize}
%     \item There must exist atleast 1 hyperplane which separates the points in the dataset based on their labels.
%     \begin{itemize}
%         \item An example of a 2D dataset which doesn't satisfy the above condition is given below.
%         \begin{center}
%             \includegraphics[scale=0.2]{"images/perc-02.png"}
%         \end{center}
%     \end{itemize}
%     \item The margin of the dataset $\gamma > 0$.
%     \begin{itemize}
%         \item Empirically, $\gamma$ should be large enough for a adequately fast training speed.
%     \end{itemize}
% \end{itemize}

% \subsection{The Model}
% Consider the training dataset $D = \{(x^i,y^i)\}_{i=1}^n$.
% To solve the above problem, our aim is to train a function/model $h: X \rightarrow Y$. For testing, given $\hat{x}$, the model will be able to predict $\hat{y} = h(\hat{x})$. \\

% \noindent Considering the perceptron model, the input is of the form $x = (x_1,x_2,x_3,...x_d) \in \mathbb{R}^d$, the associated weights are of the form $w = (w_1,w_2,w_3,...w_d) \in \mathbb{R}^d$, and the output is of the form $y \in \{+1,-1\}$. \\

% \noindent The prediction rule is 
% $$
% h(x) = 
% \begin{cases}
%     +1 & \text{if } w^Tx \geq \theta \\
%     -1 & \text{if } w^Tx \le \theta \\
% \end{cases}
% $$

% \noindent For notational purposes, we can define the following

% \begin{align*}
%     \Tilde{x} &= (1,x) = (1,x_1,x_2,...x_d) \in \mathbb{R}^{d+1} \\
%     \Tilde{w} &= (1,w) = (-\theta,w_1,w_2,...w_d) \in \mathbb{R}^{d+1}
% \end{align*}

% \noindent This implies the following modified prediction rule
% $$
% H(\Tilde{x}) = 
% \begin{cases}
%     +1 & \text{if } \Tilde{w}^T\Tilde{x} \geq 0 \\
%     -1 & \text{if } \Tilde{w}^T\Tilde{x} \le 0 \\
% \end{cases}
% $$

% \subsection{The Algorithms}

% \begin{algorithm}
% \caption{Algorithm to train a perceptron with a large dataset}
% \begin{algorithmic}
% \State $\tilde{w}^1 \gets 0$ \Comment{Initialize weights}
% \State $\text{for } t \gets 1,2,3,...N \text{ do}$
% \State \hskip1.5em receive $(x^t, y^t), x^t \in \mathbb{R}^d, y^t \in \{+1,-1\}$
% \State \hskip1.5em transform $x^t$ into $\tilde{x}^t = (1,x^t) \in \mathbb{R}^{d+1}$
% \State \hskip1.5em $\hat{y} = \text{perceptron}(\tilde{x}^t,\tilde{w}^t)$ = $sign(\langle \tilde{x}^t,\tilde{w}^t \rangle)$
% \State \hskip1.5em if $\hat{y} \neq y^t$ then
% \State \hskip3em $\tilde{w}^{t+1} = \tilde{w}^t + y^t\tilde{x}^t$
% \State \hskip1.5em else
% \State \hskip3em $\tilde{w}^{t+1} = \tilde{w}^t$
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}
% \caption{Algorithm to train a perceptron with a small dataset}
% \begin{algorithmic}
% \State $\tilde{w}^1 \gets 0$ \Comment{Initialize weights}
% \State $\text{for } t \gets 1,2,3,... \text{ do}$
% \State \hskip1.5em $\text{for } j \gets 1,2,3,...N \text{ do}$
% \State \hskip3em receive $(x^j, y^j), x^j \in \mathbb{R}^d, y^j \in \{+1,-1\}$
% \State \hskip3em transform $x^j$ into $\tilde{x}^j = (1,x^j) \in \mathbb{R}^{d+1}$
% \State \hskip3em $\hat{y} = \text{perceptron}(\tilde{x}^j,\tilde{w}^j)$ = $sign(\langle \tilde{x}^j,\tilde{w}^j \rangle)$
% \State \hskip3em if $\hat{y} \neq y^j$ then
% \State \hskip4.5em $\tilde{w}^{j+1} = \tilde{w}^j + y^j\tilde{x}^j$
% \State \hskip3em else
% \State \hskip4.5em $\tilde{w}^{j+1} = \tilde{w}^j$
% \end{algorithmic}
% \end{algorithm}

% \newpage
% \subsection{The Procedure}

% \noindent Let's try to mathematically formulate the \textbf{linear separability assumption}. \\

% \noindent \fbox{\begin{minipage}{\linewidth}
% \noindent \textbf{Linear Separability Assumption}
% \vspace{1mm}
% \hrule
% \vspace{1mm}

% \noindent Let $D = \{(x^t,y^t)\}_{t=1}^{\infty}$ denote the training data where $x^t \in \mathbb{R}^d, y^t \in \{+1,-1\} \ \forall t = 1,2,...$. \\ 
% \noindent Let $\gamma > 0$ denote the \textbf{margin of the dataset}. Then $\exists \ w^* \in \mathbb{R}^d, w^* \neq 0$, such that 
% \begin{align}
% &\langle w^*,x^t \rangle > \gamma, \text{ where } \gamma^t = 1 \\
% &\langle w^*,x^t \rangle < -\gamma, \text{ where } \gamma^t = -1 \\
% &(1.1),(1.2) \implies y^t\langle w^*,x^t \rangle > \gamma
% \end{align}
% \end{minipage}}
% \vspace{5mm}

% \noindent \textbf{Claim}: If the linear separability assumption holds, the perceptron makes a finite number of mistakes during the run of the algorithm. \\

% \noindent \textbf{Proof}: Consider a round $t$ where the perceptron made a mistake. By definition, $w^*$ depends on the dataset, so it is essentially constant in our analysis. The problem lies in figuring out the value of $w^*$. However, let's consider the difference in the dot products $\langle w^*,w^{t+1} \rangle$ and $\langle w^*,w^t \rangle$.

% \begin{align}
% \langle w^*,w^{t+1} \rangle - \langle w^*,w^t \rangle &= \langle w^*,w^t + y^tx^t \rangle - \langle w^*,w^t \rangle \\
% &= \langle w^*,y^tx^t \rangle \\
% &= y^t\langle w^*,x^t \rangle \\
% \end{align}

% \noindent Using the \textbf{linear separability assumption}, 
% \begin{align}
% (1.3) \implies y^t\langle w^*,x^t \rangle &> \gamma \\
% \implies \langle w^*,w^{t+1} \rangle - \langle w^*,w^t \rangle &> \gamma
% \end{align}

% \noindent Now, let's try doing a summation over all the dataset points.
% $$
% \overset{N}{\underset{i=1}{\sum}} \langle w^*,w^{t+1} \rangle - \langle w^*,w^t \rangle
% $$

% \noindent For the rounds having mistakes, $(1.9)$ holds, while for rounds having no mistakes, 
% $$
% w^{t+1} = w^t \implies \langle w^*,w^{t+1} \rangle - \langle w^*,w^t \rangle = 0
% $$

% \noindent Let's say the total number of rounds with mistakes is $M$. Hence, our summation changes into
% \begin{align}
% \overset{N}{\underset{i=1}{\sum}} \langle w^*,w^{t+1} \rangle - \langle w^*,w^t \rangle &= \underset{\text{mistakes}}{\sum} \langle w^*,w^{t+1} \rangle - \langle w^*,w^t \rangle \\
% \implies \overset{N}{\underset{i=1}{\sum}} \langle w^*,w^{t+1} \rangle - \langle w^*,w^t \rangle &> M\gamma \\
% \implies \langle w^*,w^{N+1} \rangle - \langle w^*,w^1 \rangle &> M\gamma \\
% w^1 = 0 \implies \langle w^*,w^{N+1} \rangle &> M\gamma \\
% \implies M\gamma < \langle w^*,w^{N+1} \rangle &\leq ||w^*||.||w^{N+1}||
% \end{align}

% \noindent For the rounds having mistakes, let's consider $||w^{t+1}||^2$. 
% \begin{align}
% ||w^{t+1}||^2 &= ||w^t + y^tx^t||^2 \\
% &= (w^t+y^tx^t)^T(w^t+y^tx^t) \\
% &= ||w^t||^2 + ||y^t||^2.||x^t||^2 + 2y^t\langle w^t,x^t \rangle
% \end{align}

% \newpage
% \noindent We know that the last term is the product of $y^t$ and the prediction value $\hat{y}$. Since this round has the prediction mistake, we can conclude that the last term is \textbf{negative}. Also, we know that $||y^t||^2 = 1$.
% \begin{align}
% ||w^{t+1}||^2 - ||w^t||^2 &= ||y^t||^2.||x^t||^2 + 2y^t\langle w^t,x^t \rangle \\
% &< ||x^t||^2
% \end{align}

% \noindent Performing a telescopic summation over all rounds with mistakes, and using $w^1 = 0$ yields
% \begin{align}
% ||w^{N+1}||^2 < M||x^t||^2
% \end{align}

% \noindent Realistically, all the points in the dataset can be bounded within a d-dimensional ball, whose radius $R$ is the max L2-norm among all the feature vectors $x^i, i = 1,2,3,...N$.
% \begin{align}
% ||w^{N+1}||^2 < M||x^t||^2 &< MR^2 \\
% \implies ||w^{N+1}|| < R\sqrt{M}
% \end{align}

% \noindent Using $(1.14),(1.22)$, we obtain
% \begin{align}
% M\gamma < ||w^*||.||w^{N+1}|| &< R\sqrt{M}||w^*|| \\
% \implies M < ||w^*||^2\cdot R^2/\gamma^2
% \end{align}

% \noindent To summarize, the following inequality, also known as the \textbf{Perceptron Mistake Bound}, implies an upper bound on the number of mistakes $M$ occuring during the training of a perceptron.
% $$
% \boxed{M < ||w^*||^2\cdot R^2/\gamma^2}
% $$
